{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded openai api key.\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter the file path or URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = input(\"Enter the file path or URL: \")\n",
    "text_path = \"example/example.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorize the data, the same document will only be processed once and cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cached memories.\n"
     ]
    }
   ],
   "source": [
    "memory_path = memorize(text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question: What is this article about?\n",
      "\n",
      "This article discusses a new neural network architecture called the Transformer, which uses attention mechanisms instead of recurrent or convolutional neural networks for sequence transduction tasks such as language modeling and machine translation. The Transformer is found to be superior in quality, more parallelizable, and requiring significantly less time to train compared to existing models. The article also discusses experiments on English constituency parsing which show that the Transformer can generalize well to other tasks.\n",
      "\n",
      "Enter your question: 这篇文章讲了什么？\n",
      "\n",
      "这篇文章介绍了一种新的网络结构，Transformer，完全基于注意力机制，取代了编码器和解码器中常用的复杂循环神经网络或卷积神经网络。Transformer在机器翻译等序列转导任务中表现优异，在更短的训练时间内实现了更好的性能，并具有更高的并行性。作者还展示了Transformer对英语组成分分析等其他任务的泛化能力。文章还提供了注意力可视化示例，展示了不同注意力头的学习任务和行为。\n",
      "\n",
      "Enter your question: How self-attention is calculated?\n",
      "\n",
      "Self-attention is calculated through a multi-head self-attention mechanism. The queries, keys, and values are linearly projected h times with different, learned linear projections to dk, dk, and dv dimensions, respectively. The model then performs attention on each projected version of queries, keys, and values in parallel, yielding dv-dimensional output values. These output values are then concatenated and projected again to obtain the final values. The attention function maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. The particular attention used is Scaled Dot-Product Attention, where the dot products of the query with all keys are computed, divided by √dk, and a softmax function is applied to obtain the weights on the values. Each layer in the encoder and decoder has two sub-layers, the first being the multi-head self-attention mechanism and the second a position-wise fully connected feed-forward network.\n",
      "\n",
      "Enter your question: 文中提出的自注意力是如何计算的？\n",
      "\n",
      "The self-attention mechanism in the Transformer model is calculated using queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed, then each is divided by √dk, and a softmax function is applied to obtain weights on the values. The attention function is computed on a set of queries simultaneously, packed together into a matrix Q, with keys and values packed into matrices K and V. The output is computed as a weighted sum of the values using the weights obtained from the softmax function. The self-attention mechanism has advantages over recurrent and convolutional layers in terms of computational complexity, amount of computation that can be parallelized, and path length between long-range dependencies in the network. The Transformer model uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The positional encoding used in the model corresponds to a sinusoid, allowing the model to learn to attend by relative positions. Attention distributions from the models can be inspected for interpretability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat(memory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
